# src/model/enhancer_v2.py
# âœ… å³æ’å³ç”¨çš„é€šç”¨ç‰¹å¾å¢å¼ºå™¨
# ä½œè€…: WangWeiqiang-UCAS
# æ—¥æœŸ: 2025-11-07

"""
GeneralPurposeEnhancer_V2: å³æ’å³ç”¨çš„ç‰¹å¾å¢å¼ºæ¨¡å—

è®¾è®¡ç†å¿µ:
    - è¾“å…¥: åŸå§‹å›¾åƒ + ä»»æ„éª¨å¹²ç½‘ç»œè¾“å‡ºçš„ç‰¹å¾é‡‘å­—å¡”
    - è¾“å‡º: å¢å¼ºåçš„ç‰¹å¾é‡‘å­—å¡” (ç›¸åŒshape)
    - é»‘ç›’æ“ä½œ: æ— éœ€äº†è§£å†…éƒ¨çš„ SPM/GRM/VQ å®ç°ç»†èŠ‚

ä½¿ç”¨åœºæ™¯:
    1. ç›®æ ‡æ£€æµ‹: YOLO/Faster-RCNN/DETR ç­‰
    2. è¯­ä¹‰åˆ†å‰²: FCN/UNet/DeepLab ç­‰
    3. å®ä¾‹åˆ†å‰²: Mask-RCNN/SOLO ç­‰
    4. å›¾åƒæ¢å¤: å»å™ª/å»é›¨/å»é›¾ ç­‰

ç¤ºä¾‹:
    >>> enhancer = GeneralPurposeEnhancer_V2(
    ...     feature_channels=[64, 128, 256],  # éª¨å¹²ç½‘ç»œè¾“å‡ºé€šé“
    ...     img_size=640,                      # è¾“å…¥å›¾åƒå°ºå¯¸
    ... )
    >>>
    >>> # ä½¿ç”¨
    >>> image = torch.randn(1, 3, 640, 640)
    >>> features = backbone(image)  # [(1,64,H,W), (1,128,H/2,W/2), (1,256,H/4,W/4)]
    >>> enhanced_features, info = enhancer(image, features)
    >>> # enhanced_features ä¸ features å½¢çŠ¶å®Œå…¨ç›¸åŒ
"""

from typing import List, Dict, Tuple, Optional
import torch
import torch.nn as nn
import torch.nn.functional as F

from .spm_v2 import ScenePromptModule_V2
from .grm_v2 import VQFeatureRefinementModule_V2


class GeneralPurposeEnhancer_V2(nn.Module):
    """
    é€šç”¨ç‰¹å¾å¢å¼ºå™¨ V2

    æ ¸å¿ƒåŠŸèƒ½:
        1. è‡ªåŠ¨åˆ†æåœºæ™¯é€€åŒ–ç±»å‹ (SPM)
        2. ç”Ÿæˆç¦»æ•£åŒ–çš„ä¿®å¤æŒ‡ä»¤åºåˆ— (VQ)
        3. é€šè¿‡äº¤å‰æ³¨æ„åŠ›ä¿®å¤ç‰¹å¾ (GRM)
        4. æ”¯æŒå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”
        5. å®Œå…¨å³æ’å³ç”¨ï¼Œæ— éœ€ä¿®æ”¹ä¸»å¹²ç½‘ç»œ
    """

    def __init__(
            self,
            feature_channels: List[int],
            grm_shared_channels: int = 128,
            prompt_dim: int = 128,
            img_size: int = 256,
            patch_size: int = 16,
            spm_num_conditions: int = 9,  # æ”¯æŒçš„é€€åŒ–ç±»å‹æ•°é‡
            vq_num_embeddings: int = 512,  # æç¤ºç æœ¬å¤§å°
            vq_commitment_cost: float = 0.25,
            freeze_grm: bool = False,
            verbose: bool = True
    ):
        """
        åˆå§‹åŒ–å¢å¼ºå™¨

        Args:
            feature_channels: ç‰¹å¾é‡‘å­—å¡”å„å±‚çš„é€šé“æ•°ï¼Œä¾‹å¦‚ [64, 128, 256]
            grm_shared_channels: GRM å†…éƒ¨ç»Ÿä¸€å¤„ç†çš„é€šé“æ•°
            prompt_dim: æç¤ºå‘é‡çš„ç»´åº¦
            img_size: è¾“å…¥å›¾åƒçš„å°ºå¯¸ (å‡è®¾æ­£æ–¹å½¢)
            patch_size: å°†å›¾åƒåˆ†æˆ patch çš„å¤§å°
            spm_num_conditions: æ”¯æŒçš„é€€åŒ–ç±»å‹æ•°é‡
            vq_num_embeddings: ç¦»æ•£æç¤ºç æœ¬çš„å¤§å°
            vq_commitment_cost: VQ æŸå¤±çš„æƒé‡
            freeze_grm: æ˜¯å¦å†»ç»“ GRMï¼ˆå³æ’å³ç”¨æ¨¡å¼ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        super().__init__()

        self.feature_channels = feature_channels
        self.grm_shared_channels = grm_shared_channels
        self.prompt_dim = prompt_dim
        self.img_size = img_size
        self.patch_size = patch_size
        self.verbose = verbose

        # ========================================
        # 1. åœºæ™¯åˆ†ææ¨¡å— (SPM)
        # ========================================
        # åŠŸèƒ½: åˆ†æå›¾åƒçš„é€€åŒ–ç±»å‹ï¼Œç”Ÿæˆç¦»æ•£åŒ–çš„ä¿®å¤æŒ‡ä»¤
        self.spm = ScenePromptModule_V2(
            input_channels=3,
            img_size=img_size,
            patch_size=patch_size,
            prompt_dim=prompt_dim,
            num_conditions=spm_num_conditions,
            vq_num_embeddings=vq_num_embeddings,
            vq_commitment_cost=vq_commitment_cost
        )

        # ========================================
        # 2. ç‰¹å¾ä¿®å¤æ¨¡å— (GRM)
        # ========================================
        # åŠŸèƒ½: åŸºäºä¿®å¤æŒ‡ä»¤ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¿®å¤ç‰¹å¾
        self.grm = VQFeatureRefinementModule_V2(
            channels=grm_shared_channels,
            prompt_dim=prompt_dim,
            num_heads=4
        )

        if freeze_grm:
            self.grm.freeze()

        # ========================================
        # 3. è‡ªé€‚åº”æŠ•å½±å±‚
        # ========================================
        # åŠŸèƒ½: ä½¿å¢å¼ºå™¨èƒ½å¤Ÿå¤„ç†ä»»æ„é€šé“æ•°çš„ç‰¹å¾
        # è¾“å…¥æŠ•å½±: å°†ç‰¹å¾ç»Ÿä¸€åˆ° grm_shared_channels
        self.grm_in_projs = nn.ModuleList([
            nn.Conv2d(in_ch, grm_shared_channels, kernel_size=1, bias=False)
            if in_ch != grm_shared_channels else nn.Identity()
            for in_ch in feature_channels
        ])

        # è¾“å‡ºæŠ•å½±: å°†ä¿®å¤åçš„ç‰¹å¾æ¢å¤åˆ°åŸå§‹é€šé“æ•°
        self.grm_out_projs = nn.ModuleList([
            nn.Conv2d(grm_shared_channels, out_ch, kernel_size=1, bias=False)
            if out_ch != grm_shared_channels else nn.Identity()
            for out_ch in feature_channels
        ])

        # åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡
        self._init_projection_weights()

        if verbose:
            self._print_info()

    def _init_projection_weights(self):
        """åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡"""
        for module in self.grm_in_projs:
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')

        for module in self.grm_out_projs:
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')

    def _print_info(self):
        """æ‰“å°æ¨¡å—ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        print(f"\n{'=' * 80}")
        print(f"{'âœ… GeneralPurposeEnhancer V2 - å³æ’å³ç”¨ç‰¹å¾å¢å¼ºå™¨':^80}")
        print(f"{'=' * 80}")
        print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"  è¾“å…¥å›¾åƒå°ºå¯¸:        {self.img_size} x {self.img_size}")
        print(f"  Patch å¤§å°:          {self.patch_size} x {self.patch_size}")
        print(f"  Patch æ•°é‡:          {(self.img_size // self.patch_size) ** 2}")
        print(f"  ç‰¹å¾é‡‘å­—å¡”é€šé“:      {self.feature_channels}")
        print(f"  GRM ç»Ÿä¸€é€šé“:        {self.grm_shared_channels}")
        print(f"  æç¤ºå‘é‡ç»´åº¦:        {self.prompt_dim}")
        print(f"  ç¦»æ•£ç æœ¬å¤§å°:        {self.spm.vq_layer.num_embeddings}")
        print(f"  æ”¯æŒé€€åŒ–ç±»å‹:        {self.spm.multi_label_head.out_features}")

        print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°é‡:            {total_params / 1e6:.2f}M")
        print(f"  å¯è®­ç»ƒå‚æ•°:          {trainable_params / 1e6:.2f}M")
        print(f"  å†»ç»“å‚æ•°:            {(total_params - trainable_params) / 1e6:.2f}M")

        print(f"\nğŸ”§ æ¨¡å—ç»„æˆ:")
        spm_params = sum(p.numel() for p in self.spm.parameters())
        grm_params = sum(p.numel() for p in self.grm.parameters())
        proj_params = (sum(p.numel() for p in self.grm_in_projs.parameters()) +
                       sum(p.numel() for p in self.grm_out_projs.parameters()))
        print(f"  SPM (åœºæ™¯åˆ†æ):      {spm_params / 1e6:.2f}M ({spm_params / total_params * 100:.1f}%)")
        print(f"  GRM (ç‰¹å¾ä¿®å¤):      {grm_params / 1e6:.2f}M ({grm_params / total_params * 100:.1f}%)")
        print(f"  æŠ•å½±å±‚ (è‡ªé€‚åº”):     {proj_params / 1e6:.2f}M ({proj_params / total_params * 100:.1f}%)")

        print(f"\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print(f"  1. è¾“å…¥: image (B,3,H,W) + feature_pyramid [List of Tensors]")
        print(f"  2. è¾“å‡º: enhanced_features (ç›¸åŒshape) + info (Dict)")
        print(f"  3. å³æ’å³ç”¨: å¯æ¥å…¥ä»»æ„éª¨å¹²ç½‘ç»œ (YOLO/ResNet/ViT/...)")
        print(f"  4. å¯æ§ç¼–è¾‘: é€šè¿‡æ“çºµ prompt_indices å®ç°ç²¾ç¡®æ§åˆ¶")
        print(f"{'=' * 80}\n")

    def forward(
            self,
            image: torch.Tensor,
            feature_pyramid: List[torch.Tensor],
            use_vq: bool = True,
            custom_prompt_indices: Optional[torch.Tensor] = None
    ) -> Tuple[List[torch.Tensor], Dict]:
        """
        å‰å‘ä¼ æ’­

        Args:
            image: åŸå§‹è¾“å…¥å›¾åƒ (B, 3, H, W)
            feature_pyramid: éª¨å¹²ç½‘ç»œè¾“å‡ºçš„ç‰¹å¾é‡‘å­—å¡”
                æ ¼å¼: [(B, C1, H1, W1), (B, C2, H2, W2), ...]
            use_vq: æ˜¯å¦ä½¿ç”¨å‘é‡é‡åŒ–ï¼ˆè®­ç»ƒæ—¶å»ºè®®Trueï¼Œæ¨ç†æ—¶å¯é€‰ï¼‰
            custom_prompt_indices: è‡ªå®šä¹‰çš„æç¤ºç ç´¢å¼• (B, N)
                ç”¨äºå¯æ§ç¼–è¾‘ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨å®ƒè€Œä¸æ˜¯è‡ªåŠ¨åˆ†æ

        Returns:
            enhanced_features: å¢å¼ºåçš„ç‰¹å¾é‡‘å­—å¡” (ä¸è¾“å…¥ç›¸åŒshape)
            info: è¾…åŠ©ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«:
                - 'prompt_seq': æç¤ºåºåˆ— (B, N, D)
                - 'prompt_indices': ç¦»æ•£ç ç´¢å¼• (B, N)
                - 'condition_logits': é€€åŒ–ç±»å‹åˆ†ç±» logits (B, num_conditions)
                - 'vq_loss': VQ æŸå¤± (å¦‚æœ use_vq=True)
                - 'perplexity': å›°æƒ‘åº¦
                - 'unique_codes': ä½¿ç”¨çš„å”¯ä¸€ç æ•°é‡

        ç¤ºä¾‹:
            >>> enhancer = GeneralPurposeEnhancer_V2([64, 128, 256])
            >>> image = torch.randn(4, 3, 256, 256)
            >>> features = [
            ...     torch.randn(4, 64, 32, 32),
            ...     torch.randn(4, 128, 16, 16),
            ...     torch.randn(4, 256, 8, 8)
            ... ]
            >>> enhanced, info = enhancer(image, features)
            >>> print([f.shape for f in enhanced])
            [(4, 64, 32, 32), (4, 128, 16, 16), (4, 256, 8, 8)]
        """
        # ========================================
        # é˜¶æ®µ 1: åœºæ™¯åˆ†æä¸æç¤ºç”Ÿæˆ
        # ========================================
        with torch.set_grad_enabled(self.spm.training):
            spm_output = self.spm(image, use_vq=use_vq)

        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰ç´¢å¼•ï¼Œåˆ™ä»ç´¢å¼•é‡å»ºæç¤ºåºåˆ—
        if custom_prompt_indices is not None:
            prompt_seq = self.spm.reconstruct_from_indices(custom_prompt_indices)
            prompt_indices = custom_prompt_indices
        else:
            prompt_seq = spm_output['prompt_seq']
            prompt_indices = spm_output.get('prompt_indices')

        # ========================================
        # é˜¶æ®µ 2: ç‰¹å¾ä¿®å¤
        # ========================================
        enhanced_features = []

        for i, feature_map in enumerate(feature_pyramid):
            # 2.1 æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
            projected_feat = self.grm_in_projs[i](feature_map)

            # 2.2 GRM ä¿®å¤ (åŸºäºäº¤å‰æ³¨æ„åŠ›)
            feature_delta, _ = self.grm(projected_feat, prompt_seq)

            # 2.3 æŠ•å½±å›åŸå§‹ç»´åº¦
            refined_delta = self.grm_out_projs[i](feature_delta)

            # 2.4 æ®‹å·®è¿æ¥
            enhanced_feat = feature_map + refined_delta
            enhanced_features.append(enhanced_feat)

        # ========================================
        # é˜¶æ®µ 3: æ”¶é›†è¾…åŠ©ä¿¡æ¯
        # ========================================
        info = {
            'prompt_seq': prompt_seq,
            'prompt_indices': prompt_indices,
            'condition_logits': spm_output['condition_logits'],
            'scene_analysis': spm_output,  # å®Œæ•´çš„ SPM è¾“å‡º
        }

        # æ·»åŠ  VQ ç›¸å…³æŒ‡æ ‡
        if 'vq_loss' in spm_output:
            info['vq_loss'] = spm_output['vq_loss']
            info['perplexity'] = spm_output.get('perplexity', torch.tensor(0.0))
            info['unique_codes'] = spm_output.get('unique_codes', 0)

        return enhanced_features, info

    def freeze(self):
        """å†»ç»“æ‰€æœ‰å‚æ•°ï¼ˆå³æ’å³ç”¨æ¨¡å¼ï¼‰"""
        for param in self.parameters():
            param.requires_grad = False
        if self.verbose:
            print("ğŸ”’ Enhancer V2 å·²å†»ç»“ï¼ˆå³æ’å³ç”¨æ¨¡å¼ï¼‰")

    def unfreeze(self):
        """è§£å†»æ‰€æœ‰å‚æ•°ï¼ˆå¾®è°ƒæ¨¡å¼ï¼‰"""
        for param in self.parameters():
            param.requires_grad = True
        if self.verbose:
            print("ğŸ”“ Enhancer V2 å·²è§£å†»ï¼ˆå¾®è°ƒæ¨¡å¼ï¼‰")

    def freeze_spm_only(self):
        """ä»…å†»ç»“ SPMï¼ˆä¿æŒåœºæ™¯åˆ†æèƒ½åŠ›ï¼Œåªè®­ç»ƒ GRMï¼‰"""
        for param in self.spm.parameters():
            param.requires_grad = False
        if self.verbose:
            print("ğŸ”’ SPM å·²å†»ç»“ï¼ŒGRM ä¿æŒå¯è®­ç»ƒ")

    def freeze_grm_only(self):
        """ä»…å†»ç»“ GRMï¼ˆä¿æŒä¿®å¤èƒ½åŠ›ï¼Œåªè®­ç»ƒ SPMï¼‰"""
        for param in self.grm.parameters():
            param.requires_grad = False
        for proj in self.grm_in_projs:
            for param in proj.parameters():
                param.requires_grad = False
        for proj in self.grm_out_projs:
            for param in proj.parameters():
                param.requires_grad = False
        if self.verbose:
            print("ğŸ”’ GRM å·²å†»ç»“ï¼ŒSPM ä¿æŒå¯è®­ç»ƒ")

    def get_codebook_usage_stats(self) -> Dict:
        """è·å–ç æœ¬ä½¿ç”¨ç»Ÿè®¡"""
        code_usage = self.spm.vq_layer.code_usage.cpu()
        total_codes = len(code_usage)
        used_codes = (code_usage > 0).sum().item()

        return {
            'total_codes': total_codes,
            'used_codes': used_codes,
            'usage_rate': used_codes / total_codes * 100,
            'code_distribution': code_usage.numpy()
        }

    def manipulate_prompt(
            self,
            original_indices: torch.Tensor,
            operation: str = 'replace',
            target_code: Optional[int] = None,
            source_code: Optional[int] = None,
            positions: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å‡†ç¬¦å·åŒ–æ“ä½œ: æ“çºµæç¤ºç åºåˆ—

        Args:
            original_indices: åŸå§‹çš„ç æœ¬ç´¢å¼• (B, N)
            operation: æ“ä½œç±»å‹ ('replace', 'add', 'remove')
            target_code: ç›®æ ‡ç 
            source_code: æºç ï¼ˆä»…ç”¨äº replaceï¼‰
            positions: æ“ä½œä½ç½®çš„ mask (B, N)

        Returns:
            new_indices: ä¿®æ”¹åçš„ç´¢å¼•

        ç¤ºä¾‹:
            >>> # å°†å·¦ä¸Šè§’åŒºåŸŸå¼ºåˆ¶ä½¿ç”¨"å¼ºå»é›¨"ç­–ç•¥
            >>> mask = create_topleft_mask(batch_size, num_patches)
            >>> new_indices = enhancer.manipulate_prompt(
            ...     original_indices,
            ...     operation='add',
            ...     target_code=100,  # å‡è®¾ 100 ä»£è¡¨å¼ºå»é›¨
            ...     positions=mask
            ... )
        """
        return self.spm.manipulate_prompt(
            original_indices,
            operation=operation,
            target_code=target_code,
            source_code=source_code,
            positions=positions
        )

    @property
    def device(self):
        """è¿”å›æ¨¡å‹æ‰€åœ¨è®¾å¤‡"""
        return next(self.parameters()).device


# ============================================
# ä¾¿æ·æ„å»ºå‡½æ•°
# ============================================

def build_enhancer_for_yolo(
    img_size: int = 640,
    pretrained_path: Optional[str] = None,
    freeze: bool = False
) -> GeneralPurposeEnhancer_V2:
    """
    ä¸º YOLO ç³»åˆ—æ„å»ºå¢å¼ºå™¨
    
    Args:
        img_size: è¾“å…¥å›¾åƒå°ºå¯¸
        pretrained_path: é¢„è®­ç»ƒæƒé‡è·¯å¾„
        freeze: æ˜¯å¦å†»ç»“ï¼ˆå³æ’å³ç”¨ï¼‰
    """
    enhancer = GeneralPurposeEnhancer_V2(
        feature_channels=[64, 128, 256],
        grm_shared_channels=128,
        prompt_dim=128,
        img_size=img_size,
        patch_size=16,
        spm_num_conditions=9,
        vq_num_embeddings=512
    )
    
    if pretrained_path:
        checkpoint = torch.load(pretrained_path, map_location='cpu')
        if 'enhancer_state_dict' in checkpoint:
            state_dict = checkpoint['enhancer_state_dict']
            
            # âœ… ä¿®å¤ï¼šå¤„ç†ä½ç½®ç¼–ç å°ºå¯¸ä¸åŒ¹é…
            current_pos_embed_shape = enhancer.spm.pos_embed.shape  # [1, 1600, 128]
            pretrained_pos_embed_shape = state_dict['spm.pos_embed'].shape  # [1, 256, 128]
            
            if current_pos_embed_shape != pretrained_pos_embed_shape:
                print(f"\nâš ï¸  ä½ç½®ç¼–ç å°ºå¯¸ä¸åŒ¹é…:")
                print(f"   é¢„è®­ç»ƒ: {pretrained_pos_embed_shape}")
                print(f"   å½“å‰:   {current_pos_embed_shape}")
                print(f"   ğŸ”§ ä½¿ç”¨æ’å€¼è°ƒæ•´ä½ç½®ç¼–ç ...\n")
                
                # æ’å€¼ä½ç½®ç¼–ç 
                pretrained_pos_embed = state_dict['spm.pos_embed']  # [1, 256, 128]
                
                # è·å–é¢„è®­ç»ƒå’Œå½“å‰çš„ patch æ•°é‡
                pretrained_num_patches = pretrained_pos_embed_shape[1]  # 256
                current_num_patches = current_pos_embed_shape[1]  # 1600
                
                # è®¡ç®—ç½‘æ ¼å°ºå¯¸
                pretrained_grid_size = int(pretrained_num_patches ** 0.5)  # 16
                current_grid_size = int(current_num_patches ** 0.5)  # 40
                
                # é‡å¡‘ä¸º 2D ç½‘æ ¼
                pretrained_pos_embed = pretrained_pos_embed.reshape(
                    1, pretrained_grid_size, pretrained_grid_size, -1
                ).permute(0, 3, 1, 2)  # [1, 128, 16, 16]
                
                # åŒçº¿æ€§æ’å€¼åˆ°æ–°å°ºå¯¸
                import torch.nn.functional as F
                interpolated_pos_embed = F.interpolate(
                    pretrained_pos_embed,
                    size=(current_grid_size, current_grid_size),
                    mode='bicubic',  # ä½¿ç”¨åŒä¸‰æ¬¡æ’å€¼ï¼ˆæ›´å¹³æ»‘ï¼‰
                    align_corners=False
                )  # [1, 128, 40, 40]
                
                # é‡å¡‘å› [1, N, D]
                interpolated_pos_embed = interpolated_pos_embed.permute(0, 2, 3, 1).reshape(
                    1, current_num_patches, -1
                )  # [1, 1600, 128]
                
                # æ›¿æ¢ state_dict ä¸­çš„ä½ç½®ç¼–ç 
                state_dict['spm.pos_embed'] = interpolated_pos_embed
                
                print(f"   âœ… ä½ç½®ç¼–ç å·²æ’å€¼: {pretrained_pos_embed_shape} â†’ {current_pos_embed_shape}\n")
            
            # åŠ è½½æƒé‡
            enhancer.load_state_dict(state_dict)
            print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}\n")
        else:
            print(f"âš ï¸  Checkpoint ä¸­æœªæ‰¾åˆ° 'enhancer_state_dict'\n")
    
    if freeze:
        enhancer.freeze()
    
    return enhancer

def build_enhancer_for_resnet(
        img_size: int = 224,
        pretrained_path: Optional[str] = None,
        freeze: bool = False
) -> GeneralPurposeEnhancer_V2:
    """ä¸º ResNet ç³»åˆ—æ„å»ºå¢å¼ºå™¨"""
    enhancer = GeneralPurposeEnhancer_V2(
        feature_channels=[256, 512, 1024],  # ResNet çš„ C3, C4, C5
        grm_shared_channels=256,
        prompt_dim=128,
        img_size=img_size,
        patch_size=14,
        spm_num_conditions=9,
        vq_num_embeddings=512
    )

    if pretrained_path:
        checkpoint = torch.load(pretrained_path, map_location='cpu')
        if 'enhancer_state_dict' in checkpoint:
            enhancer.load_state_dict(checkpoint['enhancer_state_dict'])
            print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")

    if freeze:
        enhancer.freeze()

    return enhancer